{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ql_core import Oriented2DGrid, QLAgent, train\n",
    "from  utils import plot_rewards, visualize_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8304d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Definição do Ambiente\n",
    "GRID_SIZE = (10, 10)\n",
    "START_STATE = (0, 0, 0)  # (x, y, psi)\n",
    "GOAL_STATE = (9, 9, 0)  # (x, y, psi)\n",
    "\n",
    "print(\"Inicializando o ambiente...\")\n",
    "environment = Oriented2DGrid(\n",
    "    grid_size=GRID_SIZE, start=START_STATE, goal=GOAL_STATE, actions_type=\"omni\"\n",
    ")\n",
    "\n",
    "# 2. Definição do Agente com hiperparâmetros\n",
    "print(\"Inicializando o agente Q-Learning...\")\n",
    "agent = QLAgent(\n",
    "    state_shape=environment.state_shape,\n",
    "    n_actions=environment.n_actions,\n",
    "    learning_rate=0.2,\n",
    "    discount_factor=0.9,\n",
    "    epsilon=1.0,\n",
    "    epsilon_decay_rate=0.99995,  # Decaimento mais lento para melhor exploração\n",
    "    min_epsilon=0.05,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Treinamento\n",
    "print(\"Iniciando o treinamento do agente...\")\n",
    "rewards_history = train(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    n_episodes=25000, # Mais episódios para garantir a convergência\n",
    "    max_steps_per_episode=200,\n",
    "    verbose=True,\n",
    "    verbose_interval=2500\n",
    ")\n",
    "print(\"\\nTreinamento concluído.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92103e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualização dos Resultados\n",
    "print(\"Gerando visualizações...\")\n",
    "\n",
    "# Gráfico de Recompensas\n",
    "plot_rewards(rewards_history)\n",
    "\n",
    "# Mapa da Política Aprendida\n",
    "visualize_policy(agent, GRID_SIZE, GOAL_STATE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
