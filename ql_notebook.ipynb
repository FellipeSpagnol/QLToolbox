{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d602ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Class & Function import\n",
    "from ql_core import Agent, Environment, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8304d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 3, 1)\n",
      "[[[[0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]]]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Importa as classes e a função de treinamento do outro arquivo\n",
    "from ql_core import Environment, Agent, train\n",
    "\n",
    "\n",
    "def plot_rewards_history(rewards_history: List[float], window_size: int = 100):\n",
    "    \"\"\"\n",
    "    Plota o histórico de recompensas usando uma média móvel para suavizar o gráfico.\n",
    "\n",
    "    Args:\n",
    "        rewards_history: Lista com a recompensa total de cada episódio.\n",
    "        window_size: O tamanho da janela para calcular a média móvel.\n",
    "    \"\"\"\n",
    "    # Calcula a média móvel para suavizar a curva de aprendizado\n",
    "    moving_averages = []\n",
    "    for i in range(len(rewards_history) - window_size + 1):\n",
    "        window = rewards_history[i : i + window_size]\n",
    "        moving_averages.append(sum(window) / window_size)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(moving_averages)\n",
    "    plt.title(\"Histórico de Recompensas por Episódio (Média Móvel)\")\n",
    "    plt.xlabel(f\"Episódios (janela de {window_size})\")\n",
    "    plt.ylabel(\"Recompensa Média\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_policy(agent: Agent, environment: Environment, psi_layer_to_show: int):\n",
    "    \"\"\"\n",
    "    Cria uma visualização da política aprendida pelo agente para uma orientação específica.\n",
    "\n",
    "    Args:\n",
    "        agent: O agente treinado.\n",
    "        environment: A instância do ambiente.\n",
    "        psi_layer_to_show: O índice da camada de orientação (psi) a ser visualizada.\n",
    "    \"\"\"\n",
    "    grid_shape = (environment.state_shape[0], environment.state_shape[1])\n",
    "    policy = np.zeros(grid_shape, dtype=int)\n",
    "\n",
    "    # Extrai a melhor ação para cada célula na camada de orientação especificada\n",
    "    for x in range(grid_shape[0]):\n",
    "        for y in range(grid_shape[1]):\n",
    "            state = (x, y, psi_layer_to_show)\n",
    "            policy[x, y] = agent.choose_action(state)\n",
    "\n",
    "    # Mapeia os índices de ação para vetores (dx, dy) ou símbolos\n",
    "    # Este mapeamento deve corresponder à definição de ações na classe Environment\n",
    "    action_map = {\n",
    "        0: (1, 0),  # Direita\n",
    "        1: (-1, 0),  # Esquerda\n",
    "        2: (0, 1),  # Cima\n",
    "        3: (0, -1),  # Baixo\n",
    "        4: (1, 1),  # Diagonal\n",
    "        5: (-1, -1),  # Diagonal\n",
    "        6: (1, -1),  # Diagonal\n",
    "        7: (-1, 1),  # Diagonal\n",
    "        8: \"cw\",  # Rotação Horária\n",
    "        9: \"ccw\",  # Rotação Anti-horária\n",
    "    }\n",
    "\n",
    "    # Cria o grid para as setas\n",
    "    X, Y = np.meshgrid(np.arange(grid_shape[0]), np.arange(grid_shape[1]))\n",
    "    U, V = np.zeros(grid_shape), np.zeros(grid_shape)\n",
    "    rot_x, rot_y, rot_markers = [], [], []\n",
    "\n",
    "    for x in range(grid_shape[0]):\n",
    "        for y in range(grid_shape[1]):\n",
    "            action_index = policy[x, y]\n",
    "            action_viz = action_map.get(action_index)\n",
    "\n",
    "            if isinstance(action_viz, tuple):\n",
    "                U[x, y], V[x, y] = action_viz\n",
    "            else:  # Ação de rotação\n",
    "                rot_x.append(x)\n",
    "                rot_y.append(y)\n",
    "                # 'o' para rotação, poderia ser outro marcador\n",
    "                rot_markers.append(\"o\" if action_viz == \"cw\" else \"x\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    # Desenha as setas para movimentos de translação\n",
    "    ax.quiver(X, Y, U, V, pivot=\"tail\")\n",
    "\n",
    "    # Desenha marcadores para ações de rotação\n",
    "    for i, marker in enumerate(rot_markers):\n",
    "        ax.scatter(rot_x[i], rot_y[i], marker=marker, color=\"red\", s=100)\n",
    "\n",
    "    # Marca o início e o fim\n",
    "    start_state = environment._start\n",
    "    goal_state = environment._goal\n",
    "    ax.text(\n",
    "        start_state[0],\n",
    "        start_state[1],\n",
    "        \"S\",\n",
    "        fontsize=20,\n",
    "        color=\"green\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "    )\n",
    "    ax.text(\n",
    "        goal_state[0],\n",
    "        goal_state[1],\n",
    "        \"G\",\n",
    "        fontsize=20,\n",
    "        color=\"blue\",\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(np.arange(-0.5, grid_shape[0], 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, grid_shape[1], 1), minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "    ax.set_xlim(-0.5, grid_shape[0] - 0.5)\n",
    "    ax.set_ylim(-0.5, grid_shape[1] - 0.5)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.set_title(f\"Política Aprendida (para orientação psi = {psi_layer_to_show})\")\n",
    "    plt.gca().invert_yaxis()  # Inverte o eixo Y para corresponder ao sistema de coordenadas de matriz\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- 1. Configuração do Ambiente e do Agente ---\n",
    "    GRID_SIZE = (10, 10)\n",
    "    START_STATE = (0, 0, 0)\n",
    "    GOAL_STATE = (9, 9, 0)\n",
    "    ACTIONS_TYPE = \"omni\"\n",
    "\n",
    "    # Instancia o ambiente\n",
    "    env = Environment(\n",
    "        grid_size=GRID_SIZE,\n",
    "        start=START_STATE,\n",
    "        goal=GOAL_STATE,\n",
    "        actions_type=ACTIONS_TYPE,\n",
    "    )\n",
    "\n",
    "    # Instancia o agente\n",
    "    agent = Agent(\n",
    "        state_shape=env.state_shape,\n",
    "        n_actions=env.n_actions,\n",
    "        learning_rate=0.1,\n",
    "        discount_factor=0.99,\n",
    "        exploration_rate=1.0,\n",
    "        exploration_decay_rate=0.9999,  # Decaimento mais lento para problemas mais complexos\n",
    "        min_exploration_rate=0.01,\n",
    "    )\n",
    "\n",
    "    # --- 2. Treinamento ---\n",
    "    print(\"Iniciando o treinamento do agente...\")\n",
    "    # Aumentar n_episodes pode levar a melhores resultados, mas demora mais\n",
    "    rewards = train(agent, env, n_episodes=20000, max_steps_per_episode=100)\n",
    "    print(\"Treinamento concluído.\")\n",
    "\n",
    "    # --- 3. Visualização ---\n",
    "    # Plota o histórico de recompensas\n",
    "    plot_rewards_history(rewards)\n",
    "\n",
    "    # Visualiza a política aprendida para a orientação inicial (psi = 0)\n",
    "    # Você pode mudar o último parâmetro para ver a política para outras orientações\n",
    "    visualize_policy(agent, env, psi_layer_to_show=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
