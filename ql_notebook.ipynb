{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ql_core import Oriented2DGrid, QLAgent, train\n",
    "from  utils import plot_rewards, visualize_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c432a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_obs = np.load(\"grids/grid_relatorio.npy\")\n",
    "print(grid_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8304d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Definição do Ambiente\n",
    "X_SIZE = 10\n",
    "Y_SIZE = 10\n",
    "GRID_SIZE = (X_SIZE, Y_SIZE)\n",
    "START_STATE = (0, 0, 0)  # (x, y, psi)\n",
    "GOAL_STATE = (X_SIZE-1, Y_SIZE-1, 0)  # (x, y, psi)\n",
    "\n",
    "print(\"Inicializando o ambiente...\")\n",
    "environment = Oriented2DGrid(\n",
    "    grid_size=GRID_SIZE, start=START_STATE, goal=GOAL_STATE, actions_type=\"omni\",\n",
    "    reward_gains={\"goal\": 100.0, \"invalid\": 100.0, \"step\": 1.0, \"turn\": 1.0, \"nearby_obs\": 20.0},\n",
    "    obs_grid=grid_obs\n",
    ")\n",
    "\n",
    "# 2. Definição do Agente com hiperparâmetros\n",
    "print(\"Inicializando o agente Q-Learning...\")\n",
    "agent = QLAgent(\n",
    "    state_shape=environment.state_shape,\n",
    "    n_actions=environment.n_actions,\n",
    "    learning_rate=0.2,\n",
    "    discount_factor=0.9,\n",
    "    e_greedy_type=\"linear\"\n",
    ")\n",
    "\n",
    "# 3. Treinamento\n",
    "print(\"Iniciando o treinamento do agente...\")\n",
    "data_backup = train(\n",
    "    agent=agent,\n",
    "    environment=environment,\n",
    "    n_episodes=20000,\n",
    "    verbose=True,\n",
    "    verbose_interval=1000,\n",
    ")\n",
    "print(\"\\nTreinamento concluído.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92103e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Visualização dos Resultados\n",
    "print(\"Gerando visualizações...\")\n",
    "\n",
    "# Gráfico de Recompensas\n",
    "plot_rewards(data_backup[\"rewards_history\"])\n",
    "\n",
    "# Gráfico da Taxa de Exploração (Epsilon)\n",
    "plot_rewards(data_backup[\"epsilon_history\"])\n",
    "\n",
    "# Mapa da Política Aprendida\n",
    "visualize_policy(agent, environment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
