{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d602ae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Class & Function import\n",
    "from ql_core import Agent, Environment, train\n",
    "from utils import plot_rewards_history, visualize_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc8304d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Agent.__init__() got an unexpected keyword argument 'exploration_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      8\u001b[39m env = Environment(\n\u001b[32m      9\u001b[39m     grid_size=GRID_SIZE,\n\u001b[32m     10\u001b[39m     start=START_STATE,\n\u001b[32m     11\u001b[39m     goal=GOAL_STATE,\n\u001b[32m     12\u001b[39m     actions_type=ACTIONS_TYPE\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Instancia o agente\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m agent = \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_shape\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mn_actions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiscount_factor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexploration_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexploration_decay_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9999\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Decaimento mais lento para problemas mais complexos\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_exploration_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# --- 2. Treinamento ---\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mIniciando o treinamento do agente...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Agent.__init__() got an unexpected keyword argument 'exploration_rate'"
     ]
    }
   ],
   "source": [
    "# --- 1. Configuração do Ambiente e do Agente ---\n",
    "GRID_SIZE = (10, 10)\n",
    "START_STATE = (0, 0, 0)\n",
    "GOAL_STATE = (9, 9, 0)\n",
    "ACTIONS_TYPE = \"omni\"\n",
    "\n",
    "# Instancia o ambiente\n",
    "env = Environment(\n",
    "    grid_size=GRID_SIZE,\n",
    "    start=START_STATE,\n",
    "    goal=GOAL_STATE,\n",
    "    actions_type=ACTIONS_TYPE\n",
    ")\n",
    "\n",
    "# Instancia o agente\n",
    "agent = Agent(\n",
    "    state_shape=env.state_shape,\n",
    "    n_actions=env.n_actions,\n",
    "    learning_rate=0.1,\n",
    "    discount_factor=0.99,\n",
    "    exploration_rate=1.0,\n",
    "    exploration_decay_rate=0.9999, # Decaimento mais lento para problemas mais complexos\n",
    "    min_exploration_rate=0.01\n",
    ")\n",
    "\n",
    "# --- 2. Treinamento ---\n",
    "print(\"Iniciando o treinamento do agente...\")\n",
    "# Aumentar n_episodes pode levar a melhores resultados, mas demora mais\n",
    "rewards = train(agent, env, n_episodes=20000, max_steps_per_episode=100)\n",
    "print(\"Treinamento concluído.\")\n",
    "\n",
    "# --- 3. Visualização ---\n",
    "# Plota o histórico de recompensas\n",
    "plot_rewards_history(rewards)\n",
    "\n",
    "# Visualiza a política aprendida para a orientação inicial (psi = 0)\n",
    "# Você pode mudar o último parâmetro para ver a política para outras orientações\n",
    "visualize_policy(agent, env, psi_layer_to_show=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
